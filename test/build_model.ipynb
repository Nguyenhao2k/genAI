{"cells":[{"cell_type":"markdown","id":"ecc2dc30-f8ad-498c-b6ad-c9855139ec0a","metadata":{},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"id":"a78cffb0-d82e-4c2f-ab65-f4b04d2908bf","metadata":{"collapsed":true,"executionCancelledAt":null,"executionTime":9780,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedAt":1701749659327,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install torchmultimodal-nightly","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"outputs":[],"source":["!pip install torchmultimodal-nightly"]},{"cell_type":"code","execution_count":1,"id":"1a118cdc-98cc-4d4f-9381-bb2b735844d3","metadata":{"executionCancelledAt":null,"executionTime":953,"lastExecutedAt":1701751587782,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import torch\nimport torchvision\nimport torchvision.transforms.functional as F\n\nfrom torch import nn\nfrom tqdm import tqdm\n#from torchmultimodal.diffusion_labs.models.adm_unet.adm import adm_unet\nfrom torchmultimodal.diffusion_labs.modules.adapters.cfguidance import CFGuidance\nfrom torchmultimodal.diffusion_labs.modules.losses.diffusion_hybrid_loss import DiffusionHybridLoss\nfrom torchmultimodal.diffusion_labs.samplers.ddpm import DDPModule\nfrom torchmultimodal.diffusion_labs.predictors.noise_predictor import NoisePredictor\nfrom torchmultimodal.diffusion_labs.schedules.discrete_gaussian_schedule import linear_beta_schedule, DiscreteGaussianSchedule\nfrom torchmultimodal.diffusion_labs.transforms.diffusion_transform import RandomDiffusionSteps\nfrom torchmultimodal.diffusion_labs.utils.common import DiffusionOutput"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms.functional as F\n","\n","from torch import nn\n","from tqdm import tqdm\n","from torchmultimodal.diffusion_labs.modules.adapters.cfguidance import CFGuidance\n","from torchmultimodal.diffusion_labs.modules.losses.diffusion_hybrid_loss import DiffusionHybridLoss\n","from torchmultimodal.diffusion_labs.samplers.ddpm import DDPModule\n","from torchmultimodal.diffusion_labs.predictors.noise_predictor import NoisePredictor\n","from torchmultimodal.diffusion_labs.schedules.discrete_gaussian_schedule import linear_beta_schedule, DiscreteGaussianSchedule\n","from torchmultimodal.diffusion_labs.transforms.diffusion_transform import RandomDiffusionSteps\n","from torchmultimodal.diffusion_labs.utils.common import DiffusionOutput"]},{"cell_type":"markdown","id":"a5d37ad0-fa4f-464d-bcfc-ca5b38981c66","metadata":{},"source":["# Schedule"]},{"cell_type":"code","execution_count":2,"id":"11583926-c1e6-4b1c-9573-49d625487e88","metadata":{},"outputs":[],"source":["schedule = DiscreteGaussianSchedule(linear_beta_schedule(1000))"]},{"cell_type":"markdown","id":"df881f59-2ee1-4be8-9a7b-58d90933aa12","metadata":{},"source":["# Predictor"]},{"cell_type":"code","execution_count":3,"id":"4b5f1202-287c-4fa9-89b3-dbc280e671e3","metadata":{},"outputs":[],"source":["predictor = NoisePredictor(schedule, lambda x: torch.clamp(x, -1, 1))"]},{"cell_type":"markdown","id":"9d3a2e7d-68f3-4ef8-b852-a661cb6cdd70","metadata":{},"source":["# U-Net"]},{"cell_type":"code","execution_count":4,"id":"31d74945-023a-4a2f-8dc5-592b6a146f13","metadata":{"executionCancelledAt":null,"executionTime":60,"lastExecutedAt":1701751617399,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"class DownBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, cond_channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels + cond_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU()\n        )\n        self.pooling = nn.AvgPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x, c):\n        _, _, w, h = x.size()\n        c = c.expand(-1, -1, w, h)\n        x = self.block(torch.cat([x, c], 1))\n        x_small = self.pooling(x)\n        return x, x_small\n\nclass UpBlock(nn.Module):\n    def __init__(self, inp, out):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(inp*2, out, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(out, out, kernel_size=3, padding=1),\n            nn.ReLU()\n        )\n        self.upsample = nn.Upsample(scale_factor=2)\n\n    def forward(self, x, x_small):\n        x_big = self.upsample(x_small)\n        x = torch.cat((x_big, x), dim=1)\n        x = self.block(x)\n        return x\n\nclass UNet(nn.Module):\n    def __init__(self, time_size=32, digit_size=32, steps=1000):\n        super().__init__()\n        cond_size = time_size + digit_size\n        self.conv = nn.Conv2d(1, 128, kernel_size=3, padding=1)\n        self.down = nn.ModuleList([DownBlock(128, 256, cond_size), DownBlock(256, 512, cond_size)])\n        self.bottleneck = DownBlock(512, 512, cond_size)\n        self.up = nn.ModuleList([UpBlock(512, 256), UpBlock(256, 128)])\n\n        self.variance = nn.Conv2d(128, 1, kernel_size=3, padding=1)\n        self.prediction = nn.Conv2d(128, 1, kernel_size=3, padding=1)\n        self.time_projection = nn.Embedding(steps, time_size)\n\n    def forward(self, x, t, conditional_inputs):\n        b, c, h, w = x.shape\n        timestep = self.time_projection(t).view(b, -1, 1, 1)\n        condition = conditional_inputs[\"context\"].view(b, -1, 1, 1)\n        condition = torch.cat([timestep, condition], dim=1)\n\n        x = self.conv(x)\n        self.outs = []\n        for block in self.down:\n            out, x = block(x, condition)\n            self.outs.append(out)\n        x, _ = self.bottleneck(x, condition)\n        for block in self.up:\n            x = block(self.outs.pop(), x)\n        v = self.variance(x)\n        p = self.prediction(x)\n        return DiffusionOutput(p, v)"},"outputs":[],"source":["class DownBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, cond_channels):\n","        super().__init__()\n","        self.block = nn.Sequential(\n","            nn.Conv2d(in_channels + cond_channels, out_channels, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.ReLU()\n","        )\n","        self.pooling = nn.AvgPool2d(kernel_size=2, stride=2)\n","\n","    def forward(self, x, c):\n","        _, _, w, h = x.size()\n","        c = c.expand(-1, -1, w, h)\n","        x = self.block(torch.cat([x, c], 1))\n","        x_small = self.pooling(x)\n","        return x, x_small\n","\n","class UpBlock(nn.Module):\n","    def __init__(self, inp, out):\n","        super().__init__()\n","        self.block = nn.Sequential(\n","            nn.Conv2d(inp*2, out, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(out, out, kernel_size=3, padding=1),\n","            nn.ReLU()\n","        )\n","        self.upsample = nn.Upsample(scale_factor=2)\n","\n","    def forward(self, x, x_small):\n","        x_big = self.upsample(x_small)\n","        x = torch.cat((x_big, x), dim=1)\n","        x = self.block(x)\n","        return x\n","\n","class UNet(nn.Module):\n","    def __init__(self, time_size=32, digit_size=32, steps=1000):\n","        super().__init__()\n","        cond_size = time_size + digit_size\n","        self.conv = nn.Conv2d(1, 128, kernel_size=3, padding=1)\n","        self.down = nn.ModuleList([DownBlock(128, 256, cond_size), DownBlock(256, 512, cond_size)])\n","        self.bottleneck = DownBlock(512, 512, cond_size)\n","        self.up = nn.ModuleList([UpBlock(512, 256), UpBlock(256, 128)])\n","\n","        self.variance = nn.Conv2d(128, 1, kernel_size=3, padding=1)\n","        self.prediction = nn.Conv2d(128, 1, kernel_size=3, padding=1)\n","        self.time_projection = nn.Embedding(steps, time_size)\n","\n","    def forward(self, x, t, conditional_inputs):\n","        b, c, h, w = x.shape\n","        timestep = self.time_projection(t).view(b, -1, 1, 1)\n","        condition = conditional_inputs[\"context\"].view(b, -1, 1, 1)\n","        condition = torch.cat([timestep, condition], dim=1)\n","\n","        x = self.conv(x)\n","        self.outs = []\n","        for block in self.down:\n","            out, x = block(x, condition)\n","            self.outs.append(out)\n","        x, _ = self.bottleneck(x, condition)\n","        for block in self.up:\n","            x = block(self.outs.pop(), x)\n","        v = self.variance(x)\n","        p = self.prediction(x)\n","        return DiffusionOutput(p, v)"]},{"cell_type":"markdown","id":"73db444e-04bc-4e8c-ad4b-996796553067","metadata":{},"source":["# Diffusion Model"]},{"cell_type":"code","execution_count":5,"id":"74c34e46-b89e-4f97-9562-98f397fc521b","metadata":{},"outputs":[],"source":["unet = UNet(time_size=32, digit_size=32)\n","unet = CFGuidance(unet, {\"context\": 32}, guidance=2.0)"]},{"cell_type":"code","execution_count":6,"id":"c5315b1c-c803-4a42-9d7e-0ac105754c1d","metadata":{"executionCancelledAt":null,"executionTime":122,"lastExecutedAt":1701751630882,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"model = UNet(time_size=32, digit_size=32)\nmodel = CFGuidance(model, {\"context\": 32}, guidance=2.0)\nmodel = DDPModule(model, schedule, predictor, eval_steps)"},"outputs":[],"source":["eval_steps = torch.linspace(0, 999, 250, dtype=torch.long)\n","model = DDPModule(unet, schedule, predictor, eval_steps)"]},{"cell_type":"code","execution_count":7,"id":"c795c9ad-2f0a-4530-9049-1334959b9669","metadata":{},"outputs":[],"source":["encoder = nn.Embedding(10, 32)"]},{"cell_type":"markdown","id":"1b4aedbc-cf0e-46a8-a086-9015123475e5","metadata":{},"source":["# Data"]},{"cell_type":"code","execution_count":8,"id":"edc1cdac-c45e-4a8f-b4f2-1cb14415616d","metadata":{"executionCancelledAt":null,"executionTime":12,"lastExecutedAt":1701751636793,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from torchvision.transforms import Compose, Resize, ToTensor, Lambda\n\ndiffusion_transform = RandomDiffusionSteps(schedule, batched=False)\ntransform = Compose([Resize(32),\n                     ToTensor(),\n                     Lambda(lambda x: 2*x - 1),\n                     Lambda(lambda x: diffusion_transform({\"x\": x}))])"},"outputs":[],"source":["from torchvision.transforms import Compose, Resize, ToTensor, Lambda\n","\n","diffusion_transform = RandomDiffusionSteps(schedule, batched=False)\n","# transform = Compose([Resize(32),\n","#                      ToTensor(),\n","#                      Lambda(lambda x: 2*x - 1),\n","#                      Lambda(lambda x: diffusion_transform({\"x\": x}))])\n","\n","def scale(x):\n","    return 2 * x - 1\n","\n","def apply_diffusion_transform(x):\n","    return diffusion_transform({\"x\": x})\n","\n","transform = Compose([Resize(32),\n","                     ToTensor(),\n","                     Lambda(scale),\n","                     Lambda(apply_diffusion_transform)])"]},{"cell_type":"code","execution_count":9,"id":"49067ae8-342c-488a-9cf1-9d4381d7adb1","metadata":{"collapsed":true,"executionCancelledAt":null,"executionTime":205,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedAt":1701751638894,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from torchvision.datasets import FashionMNIST\nfrom torch.utils.data import DataLoader\n\ntrain_dataset = FashionMNIST(\"fashion_mnist\", train=True, download=True, transform=transform)\ntrain_dataloader = DataLoader(train_dataset, batch_size=192, shuffle=True, num_workers=2, pin_memory=True) #192","outputsMetadata":{"0":{"height":77,"type":"stream"},"2":{"height":117,"type":"stream"},"4":{"height":117,"type":"stream"},"6":{"height":117,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to fashion_mnist\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 26421880/26421880 [00:42<00:00, 618642.89it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Extracting fashion_mnist\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to fashion_mnist\\FashionMNIST\\raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to fashion_mnist\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 29515/29515 [00:00<00:00, 149192.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting fashion_mnist\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to fashion_mnist\\FashionMNIST\\raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to fashion_mnist\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4422102/4422102 [00:04<00:00, 990598.22it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Extracting fashion_mnist\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to fashion_mnist\\FashionMNIST\\raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to fashion_mnist\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5148/5148 [00:00<00:00, 5163146.10it/s]"]},{"name":"stdout","output_type":"stream","text":["Extracting fashion_mnist\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to fashion_mnist\\FashionMNIST\\raw\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from torchvision.datasets import FashionMNIST\n","from torch.utils.data import DataLoader\n","\n","train_dataset = FashionMNIST(\"fashion_mnist\", train=True, download=True, transform=transform)\n","train_dataloader = DataLoader(train_dataset, batch_size=192, shuffle=True, num_workers=2, pin_memory=True)"]},{"cell_type":"markdown","id":"137eb307-5298-4ca7-a7fe-241ec87f28f2","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":null,"id":"a009996c-3cbf-4d35-b1bd-79d9ac0571a6","metadata":{"collapsed":false,"executionCancelledAt":null,"executionTime":268432,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1701751937147,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"epochs = 25\n\ndevice = \"cpu\"\nencoder.to(device)\nmodel.to(device)\n\noptimizer = torch.optim.AdamW(\n    [{\"params\": encoder.parameters()}, {\"params\": model.parameters()}], lr=0.0001 # .0001\n)\nh_loss = DiffusionHybridLoss(schedule)\n\n# lr_scheduler = get_cosine_schedule_with_warmup(\n#     optimizer, 1 * len(train_dataloader), 100 * len(train_dataloader)\n# ) # added for mlp\n#scaler = torch.cuda.amp.GradScaler()\n\nencoder.train()\nmodel.train()\nfor e in range(epochs):\n\tfor sample in (pbar := tqdm(train_dataloader)):\n\t\tx, c = sample\n\t\tx0, xt, noise, t, d = x[\"x\"].to(device), x[\"xt\"].to(device), x[\"noise\"].to(device), x[\"t\"].to(device), c.to(device)\n\t\toptimizer.zero_grad()\n\n\t\t#with torch.autocast(device.split(\":\")[0]):\n\t\tc = encoder(c)\n\t\tout = model(xt, t, {\"context\": c})\n\t\tloss = h_loss(out.prediction, noise, out.mean, out.log_variance, x0, xt, t)\n\n\t\tloss.backward()\n\t\toptimizer.step()\n\t\t# scaler.scale(loss).backward()\n\t\t# scaler.step(optimizer)\n\t\t# scaler.update()\n\n\t\t#lr_scheduler.step()\n\t\tpbar.set_description(f'{e+1}| Loss: {loss.item()}')","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"outputs":[],"source":["import torch\n","# Choose the GPU device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","epochs = 5\n","\n","encoder.to(device)\n","model.to(device)\n","\n","optimizer = torch.optim.AdamW(\n","    [{\"params\": encoder.parameters()}, {\"params\": model.parameters()}], lr=0.0001\n",")\n","h_loss = DiffusionHybridLoss(schedule)\n","\n","encoder.train()\n","model.train()\n","for e in range(epochs):\n","    for sample in (pbar := tqdm(train_dataloader)):\n","        x, c = sample\n","        x0, xt, noise, t, c = x[\"x\"].to(device), x[\"xt\"].to(device), x[\"noise\"].to(device), x[\"t\"].to(device), c.to(device)\n","        optimizer.zero_grad()\n","\n","        c = encoder(c)\n","        out = model(xt, t, {\"context\": c})\n","        loss = h_loss(out.prediction, noise, out.mean, out.log_variance, x0, xt, t)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        pbar.set_description(f'{e+1}| Loss: {loss.item()}')\n"]},{"cell_type":"markdown","id":"43df1f90-d76e-4ca7-916c-983b33e8c856","metadata":{},"source":["# Generate"]},{"cell_type":"code","execution_count":11,"id":"a8b7957c-49ef-4877-9670-1a4fa149c561","metadata":{"executionCancelledAt":null,"executionTime":12,"lastExecutedAt":1701752167796,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def fashion_encoder(name, num=1):\n    fashion_dict = {\"t-shirt\": 0, \"pants\": 1, \"sweater\": 2, \"dress\": 3, \"coat\": 4, \"sandal\": 5, \"shirt\": 6, \"sneaker\": 7, \"purse\": 8, \"boot\": 9}\n    idx = torch.as_tensor([fashion_dict[name] for _ in range(num)]).to(device)\n\n    encoder.eval()\n    with torch.no_grad():\n        embed = encoder(idx)\n    return embed"},"outputs":[],"source":["def fashion_encoder(name, num=1):\n","    fashion_dict = {\"t-shirt\": 0, \"pants\": 1, \"sweater\": 2, \"dress\": 3, \"coat\": 4, \n","                    \"sandal\": 5, \"shirt\": 6, \"sneaker\": 7, \"purse\": 8, \"boot\": 9}\n","    idx = torch.as_tensor([fashion_dict[name] for _ in range(num)]).to(device)\n","\n","    encoder.eval()\n","    with torch.no_grad():\n","        embed = encoder(idx)\n","    return embed"]},{"cell_type":"code","execution_count":15,"id":"74bd6de9-6d0a-4eee-88a8-a9f78f37a264","metadata":{"executionCancelledAt":null,"executionTime":280491,"lastExecutedAt":1701752585857,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"model.eval()\n\nc = fashion_encoder(\"boot\", 9)\nnoise = torch.randn(size=(9,1,32,32)).to(device)\n\nwith torch.no_grad():\n    imgs = model(noise, conditional_inputs={\"context\": c})\n\nimg_grid = torchvision.utils.make_grid(imgs, 3)\nimg = F.to_pil_image((img_grid + 1) / 2)\nimg.resize((288, 288))"},"outputs":[],"source":["model.eval()\n","\n","c = fashion_encoder(\"boot\", 9)\n","noise = torch.randn(size=(9,1,32,32)).to(device)\n","\n","with torch.no_grad():\n","    imgs = model(noise, conditional_inputs={\"context\": c})\n","\n","img_grid = torchvision.utils.make_grid(imgs, 3)\n","img = F.to_pil_image((img_grid + 1) / 2)\n","img.resize((288, 288))"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":5}
